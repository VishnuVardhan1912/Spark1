{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb02cfec",
   "metadata": {},
   "source": [
    "## Install Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b347de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# command --- !pip install pyspark\n",
    "# command --- !pip install py4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f892d",
   "metadata": {},
   "source": [
    "## Spark context "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f76b0a7",
   "metadata": {},
   "source": [
    "Sparkcontext - It is a connection to spark clustor and used to create RDD's, Accumulators, Broadcast variables on spark clustor\n",
    "\n",
    "It is an old way of entering into spark\n",
    "\n",
    "We generally use Spark Session now a days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9845a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5265526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark configuration\n",
    "# setMaster() to define cluster\n",
    "# setAll() used to define driver memory, executed memory \n",
    "\n",
    "conf = pyspark.SparkConf().setMaster(\"local\").setAppName(\"My first spark job\").setAll([(\"spark.driver.memory\", \"40g\"), (\"spark.executor.memory\", \"50g\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222e9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a object\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730ada94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-3F4VUQ3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My first spark job</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=My first spark job>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check spark context\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a8b12",
   "metadata": {},
   "source": [
    "## SQL Context"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c675681",
   "metadata": {},
   "source": [
    "To create SQL context, By this we can perform any Sql queries on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ffd2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishnu\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sqlC = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0871b415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x1a8d9dd27f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150eae7",
   "metadata": {},
   "source": [
    "# SparkSession"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3fc127",
   "metadata": {},
   "source": [
    "The entry point into all functionality is spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba586289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc44cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Python Spark Practice\").config(\"spark.driver.memory\", \"40g\").getOrCreate()\n",
    "\n",
    "# or spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f68c104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-3F4VUQ3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My first spark job</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a8d9ddc430>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58ddde",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "raw",
   "id": "146a22e2",
   "metadata": {},
   "source": [
    "RDD stands for Resilient Distributed Datasets.\n",
    "It is a collection of immutable JVM objects that allow you to perform calculations very quickly\n",
    "They are the backborn of apache spark\n",
    "In RDD dataset is distributed or splited into chunks.this makes operations on dataset very fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c7665b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f4fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize() is a method to call RDD's\n",
    "\n",
    "rdds = spark.sparkContext.parallelize([(\"Mumabai\", 1), (\"Delhi\", 2), (\"Hyderabad\", 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93225fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdds"
   ]
  },
  {
   "cell_type": "raw",
   "id": "368936ce",
   "metadata": {},
   "source": [
    "rdds is store in that memory location. data is in memory. To see data we need to get it into hard disk\n",
    "we use method collect() to bring the data to driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "649881ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mumabai', 1), ('Delhi', 2), ('Hyderabad', 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdds.collect() # used to see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93d47444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1b94f",
   "metadata": {},
   "source": [
    "## Data Frame From RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b184062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4615c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4662a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-3F4VUQ3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My first spark job</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a8d9ddc430>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d57dfa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 1.0, \"string1\", date(2021, 1,1), datetime(2021, 1, 12, 0)),\n",
    "    (2, 2.0, \"string2\", date(2021,2,1), datetime(2021, 1,2, 12, 0)),\n",
    "    (3, 3.0, \"string3\", date(2021,3,1), datetime(2021, 1,3,12,0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "933e3700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbee7043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  1.0,\n",
       "  'string1',\n",
       "  datetime.date(2021, 1, 1),\n",
       "  datetime.datetime(2021, 1, 12, 0, 0)),\n",
       " (2,\n",
       "  2.0,\n",
       "  'string2',\n",
       "  datetime.date(2021, 2, 1),\n",
       "  datetime.datetime(2021, 1, 2, 12, 0)),\n",
       " (3,\n",
       "  3.0,\n",
       "  'string3',\n",
       "  datetime.date(2021, 3, 1),\n",
       "  datetime.datetime(2021, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa3c6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, schema=[\"num\",\"float\",\"string\", \"data\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49cb4179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num: bigint, float: double, string: string, data: date, datetime: timestamp]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7323692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+----------+-------------------+\n",
      "|num|float| string|      data|           datetime|\n",
      "+---+-----+-------+----------+-------------------+\n",
      "|  1|  1.0|string1|2021-01-01|2021-01-12 00:00:00|\n",
      "|  2|  2.0|string2|2021-02-01|2021-01-02 12:00:00|\n",
      "|  3|  3.0|string3|2021-03-01|2021-01-03 12:00:00|\n",
      "+---+-----+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbad0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+----------+-------------------+\n",
      "|num|float| string|      data|           datetime|\n",
      "+---+-----+-------+----------+-------------------+\n",
      "|  1|  1.0|string1|2021-01-01|2021-01-12 00:00:00|\n",
      "+---+-----+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f3120c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num: long (nullable = true)\n",
      " |-- float: double (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to check schema\n",
    "\n",
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623a2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
